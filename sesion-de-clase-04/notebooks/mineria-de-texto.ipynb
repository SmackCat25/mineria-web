{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"id\": \"initial_id\",\n",
    "   \"metadata\": {\n",
    "    \"collapsed\": true,\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-09-10T02:48:59.304892Z\",\n",
    "     \"start_time\": \"2025-09-10T02:48:58.184442Z\"\n",
    "    }\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\"\n",
    "   ],\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 1\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-09-10T02:49:07.597140Z\",\n",
    "     \"start_time\": \"2025-09-10T02:49:03.653650Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"nltk.download('stopwords')\\n\",\n",
    "    \"nltk.download('punkt')\\n\",\n",
    "    \"nltk.download(\\\"punkt_tab\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"!py -m spacy download es_core_news_sm\"\n",
    "   ],\n",
    "   \"id\": \"a079f5fcaf966ef1\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[nltk_data] Downloading package stopwords to\\n\",\n",
    "      \"[nltk_data]     C:\\\\Users\\\\aml\\\\AppData\\\\Roaming\\\\nltk_data...\\n\",\n",
    "      \"[nltk_data]   Package stopwords is already up-to-date!\\n\",\n",
    "      \"[nltk_data] Downloading package punkt to\\n\",\n",
    "      \"[nltk_data]     C:\\\\Users\\\\aml\\\\AppData\\\\Roaming\\\\nltk_data...\\n\",\n",
    "      \"[nltk_data]   Package punkt is already up-to-date!\\n\",\n",
    "      \"[nltk_data] Downloading package punkt_tab to\\n\",\n",
    "      \"[nltk_data]     C:\\\\Users\\\\aml\\\\AppData\\\\Roaming\\\\nltk_data...\\n\",\n",
    "      \"[nltk_data]   Package punkt_tab is already up-to-date!\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Collecting es-core-news-sm==3.8.0\\n\",\n",
    "      \"  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\\n\",\n",
    "      \"     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\\n\",\n",
    "      \"     --------------------------------------  12.8/12.9 MB 67.6 MB/s eta 0:00:01\\n\",\n",
    "      \"     --------------------------------------- 12.9/12.9 MB 36.5 MB/s eta 0:00:00\\n\",\n",
    "      \"\\u001B[38;5;2m✔ Download and installation successful\\u001B[0m\\n\",\n",
    "      \"You can now load the package via spacy.load('es_core_news_sm')\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \"[notice] A new release of pip is available: 25.1.1 -> 25.2\\n\",\n",
    "      \"[notice] To update, run: C:\\\\Users\\\\aml\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\python.exe -m pip install --upgrade pip\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 2\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-09-10T02:49:07.850750Z\",\n",
    "     \"start_time\": \"2025-09-10T02:49:07.610874Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"nlp = spacy.load(\\\"es_core_news_sm\\\")\\n\",\n",
    "    \"stop_words = set(nltk.corpus.stopwords.words('spanish'))\"\n",
    "   ],\n",
    "   \"id\": \"ec20c71d46aad4d6\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 3\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-09-10T02:49:11.771703Z\",\n",
    "     \"start_time\": \"2025-09-10T02:49:11.768891Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def process_and_normalize(documents):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Combina tokenización y normalización en un solo paso.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        documents (list): Una lista de cadenas de texto.\\n\",\n",
    "    \"\\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        list: Una lista de documentos normalizados listos para vectorizar.\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    normalized_list = []\\n\",\n",
    "    \"    for doc in documents:\\n\",\n",
    "    \"        # Pasa el documento completo por el pipeline de spaCy\\n\",\n",
    "    \"        # Esto tokeniza, lematiza, y etiqueta el texto\\n\",\n",
    "    \"        processed_doc = nlp(re.sub(r'[^\\\\w\\\\s]', '', doc.lower()))\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Filtra los tokens para eliminar stopwords y no alfabéticos\\n\",\n",
    "    \"        normalized_words = [\\n\",\n",
    "    \"            token.lemma_ for token in processed_doc\\n\",\n",
    "    \"            if token.is_alpha and token.text not in stop_words\\n\",\n",
    "    \"        ]\\n\",\n",
    "    \"        # Une las palabras normalizadas en una sola cadena\\n\",\n",
    "    \"        normalized_list.append(\\\" \\\".join(normalized_words))\\n\",\n",
    "    \"    return normalized_list\"\n",
    "   ],\n",
    "   \"id\": \"fd286e76f7c4ca89\",\n",
    "   \"outputs\": [],\n",
    "   \"execution_count\": 4\n",
    "  },\n",
    "  {\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-09-10T02:49:14.322400Z\",\n",
    "     \"start_time\": \"2025-09-10T02:49:14.307078Z\"\n",
    "    }\n",
    "   },\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"corpus = [\\n\",\n",
    "    \"    \\\"El gato negro salta sobre la mesa.\\\",\\n\",\n",
    "    \"    \\\"El perro salta sobre la cerca.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"1. Corpus original:\\\")\\n\",\n",
    "    \"print(corpus)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Tokenización y Normalización ---\\n\",\n",
    "    \"normalized_corpus = process_and_normalize(corpus)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n2. Corpus normalizado (lemas):\\\")\\n\",\n",
    "    \"print(normalized_corpus)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Representación Numérica ---\\n\",\n",
    "    \"## Representación con Bag of Words (BoW)\\n\",\n",
    "    \"print(\\\"\\\\n--- Bag of Words (BoW) ---\\\")\\n\",\n",
    "    \"vectorizer_bow = CountVectorizer()\\n\",\n",
    "    \"bow_matrix = vectorizer_bow.fit_transform(normalized_corpus)\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_bow = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer_bow.get_feature_names_out())\\n\",\n",
    "    \"df_bow.index = [f\\\"Doc {i+1}\\\" for i in range(len(corpus))]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Matriz de conteos (frecuencia de palabras):\\\")\\n\",\n",
    "    \"print(df_bow)\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Representación con TF-IDF\\n\",\n",
    "    \"print(\\\"\\\\n--- TF-IDF ---\\\")\\n\",\n",
    "    \"vectorizer_tfidf = TfidfVectorizer()\\n\",\n",
    "    \"tfidf_matrix = vectorizer_tfidf.fit_transform(normalized_corpus)\\n\",\n",
    "    \"\\n\",\n",
    "    \"df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\\n\",\n",
    "    \"df_tfidf.index = [f\\\"Doc {i+1}\\\" for i in range(len(corpus))]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Matriz de valores TF-IDF:\\\")\\n\",\n",
    "    \"print(df_tfidf)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Análisis y Comparación ---\\n\",\n",
    "    \"print(\\\"\\\\n4. Comparación y conclusiones:\\\")\\n\",\n",
    "    \"print(\\\"Las matrices de BoW y TF-IDF tienen el mismo vocabulario, pero sus valores son diferentes.\\\")\\n\",\n",
    "    \"print(\\\"- BoW: Usa conteos simples. Las palabras como 'saltar' tienen el mismo peso en ambos documentos (1).\\\")\\n\",\n",
    "    \"print(\\\"- TF-IDF: Asigna un peso mayor a las palabras distintivas. 'gato' y 'negro' en el Doc 1 y 'perro' y 'cerca' en el Doc 2 tienen valores más altos, haciéndolas más importantes para la distinción de los documentos.\\\")\"\n",
    "   ],\n",
    "   \"id\": \"371b859179740906\",\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"1. Corpus original:\\n\",\n",
    "      \"['El gato negro salta sobre la mesa.', 'El perro salta sobre la cerca.']\\n\",\n",
    "      \"\\n\",\n",
    "      \"2. Corpus normalizado (lemas):\\n\",\n",
    "      \"['gato negro salta mesa', 'perro salta cerca']\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- Bag of Words (BoW) ---\\n\",\n",
    "      \"Matriz de conteos (frecuencia de palabras):\\n\",\n",
    "      \"       cerca  gato  mesa  negro  perro  salta\\n\",\n",
    "      \"Doc 1      0     1     1      1      0      1\\n\",\n",
    "      \"Doc 2      1     0     0      0      1      1\\n\",\n",
    "      \"\\n\",\n",
    "      \"--- TF-IDF ---\\n\",\n",
    "      \"Matriz de valores TF-IDF:\\n\",\n",
    "      \"          cerca      gato      mesa     negro     perro     salta\\n\",\n",
    "      \"Doc 1  0.000000  0.534046  0.534046  0.534046  0.000000  0.379978\\n\",\n",
    "      \"Doc 2  0.631667  0.000000  0.000000  0.000000  0.631667  0.449436\\n\",\n",
    "      \"\\n\",\n",
    "      \"4. Comparación y conclusiones:\\n\",\n",
    "      \"Las matrices de BoW y TF-IDF tienen el mismo vocabulario, pero sus valores son diferentes.\\n\",\n",
    "      \"- BoW: Usa conteos simples. Las palabras como 'saltar' tienen el mismo peso en ambos documentos (1).\\n\",\n",
    "      \"- TF-IDF: Asigna un peso mayor a las palabras distintivas. 'gato' y 'negro' en el Doc 1 y 'perro' y 'cerca' en el Doc 2 tienen valores más altos, haciéndolas más importantes para la distinción de los documentos.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"execution_count\": 5\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.6\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "64867a8c354ad5c5"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
