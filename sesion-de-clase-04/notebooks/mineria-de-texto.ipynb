{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-10T02:48:58.787693Z",
     "start_time": "2025-09-10T02:48:58.783056Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:49:07.755059Z",
     "start_time": "2025-09-10T02:49:02.405616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "!python -m spacy download es_core_news_sm"
   ],
   "id": "a079f5fcaf966ef1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('es_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:49:10.612189Z",
     "start_time": "2025-09-10T02:49:10.455264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = set(nltk.corpus.stopwords.words('spanish'))"
   ],
   "id": "ec20c71d46aad4d6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:49:13.142535Z",
     "start_time": "2025-09-10T02:49:13.138548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_and_normalize(documents):\n",
    "    \"\"\"\n",
    "    Combina tokenización y normalización en un solo paso.\n",
    "\n",
    "    Args:\n",
    "        documents (list): Una lista de cadenas de texto.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de documentos normalizados listos para vectorizar.\n",
    "    \"\"\"\n",
    "    normalized_list = []\n",
    "    for doc in documents:\n",
    "        # Pasa el documento completo por el pipeline de spaCy\n",
    "        # Esto tokeniza, lematiza, y etiqueta el texto\n",
    "        processed_doc = nlp(re.sub(r'[^\\w\\s]', '', doc.lower()))\n",
    "\n",
    "        # Filtra los tokens para eliminar stopwords y no alfabéticos\n",
    "        normalized_words = [\n",
    "            token.lemma_ for token in processed_doc\n",
    "            if token.is_alpha and token.text not in stop_words\n",
    "        ]\n",
    "        # Une las palabras normalizadas en una sola cadena\n",
    "        normalized_list.append(\" \".join(normalized_words))\n",
    "    return normalized_list"
   ],
   "id": "fd286e76f7c4ca89",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:50:41.234622Z",
     "start_time": "2025-09-10T02:50:41.218944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = [\n",
    "    \"El gato negro salta sobre la mesa. porque tiene hambre el gato negro. El gato negro es mi mascota y es muy amigable\",\n",
    "    \"El perro salta sobre la cerca.\"\n",
    "]\n",
    "\n",
    "print(\"1. Corpus original:\")\n",
    "print(corpus)\n",
    "\n",
    "# Tokenización y Normalización ---\n",
    "normalized_corpus = process_and_normalize(corpus)\n",
    "\n",
    "print(\"\\n2. Corpus normalizado (lemas):\")\n",
    "print(normalized_corpus)\n",
    "\n",
    "# Representación Numérica ---\n",
    "## Representación con Bag of Words (BoW)\n",
    "print(\"\\n--- Bag of Words (BoW) ---\")\n",
    "vectorizer_bow = CountVectorizer()\n",
    "bow_matrix = vectorizer_bow.fit_transform(normalized_corpus)\n",
    "\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer_bow.get_feature_names_out())\n",
    "df_bow.index = [f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    "\n",
    "print(\"Matriz de conteos (frecuencia de palabras):\")\n",
    "print(df_bow)\n",
    "\n",
    "## Representación con TF-IDF\n",
    "print(\"\\n--- TF-IDF ---\")\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(normalized_corpus)\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tfidf.get_feature_names_out())\n",
    "df_tfidf.index = [f\"Doc {i+1}\" for i in range(len(corpus))]\n",
    "\n",
    "print(\"Matriz de valores TF-IDF:\")\n",
    "print(df_tfidf)\n",
    "\n",
    "# Análisis y Comparación ---\n",
    "print(\"\\n4. Comparación y conclusiones:\")\n",
    "print(\"Las matrices de BoW y TF-IDF tienen el mismo vocabulario, pero sus valores son diferentes.\")\n",
    "print(\"- BoW: Usa conteos simples. Las palabras como 'saltar' tienen el mismo peso en ambos documentos (1).\")\n",
    "print(\"- TF-IDF: Asigna un peso mayor a las palabras distintivas. 'gato' y 'negro' en el Doc 1 y 'perro' y 'cerca' en el Doc 2 tienen valores más altos, haciéndolas más importantes para la distinción de los documentos.\")"
   ],
   "id": "371b859179740906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Corpus original:\n",
      "['El gato negro salta sobre la mesa. porque tiene hambre el gato negro. El gato negro es mi mascota y es muy amigable', 'El perro salta sobre la cerca.']\n",
      "\n",
      "2. Corpus normalizado (lemas):\n",
      "['gato negro salta mesa hambre gato negro gato negro mascota amigable', 'perro salta cerca']\n",
      "\n",
      "--- Bag of Words (BoW) ---\n",
      "Matriz de conteos (frecuencia de palabras):\n",
      "       amigable  cerca  gato  hambre  mascota  mesa  negro  perro  salta\n",
      "Doc 1         1      0     3       1        1     1      3      0      1\n",
      "Doc 2         0      1     0       0        0     0      0      1      1\n",
      "\n",
      "--- TF-IDF ---\n",
      "Matriz de valores TF-IDF:\n",
      "       amigable     cerca      gato    hambre   mascota      mesa     negro  \\\n",
      "Doc 1  0.210789  0.000000  0.632368  0.210789  0.210789  0.210789  0.632368   \n",
      "Doc 2  0.000000  0.631667  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "          perro     salta  \n",
      "Doc 1  0.000000  0.149978  \n",
      "Doc 2  0.631667  0.449436  \n",
      "\n",
      "4. Comparación y conclusiones:\n",
      "Las matrices de BoW y TF-IDF tienen el mismo vocabulario, pero sus valores son diferentes.\n",
      "- BoW: Usa conteos simples. Las palabras como 'saltar' tienen el mismo peso en ambos documentos (1).\n",
      "- TF-IDF: Asigna un peso mayor a las palabras distintivas. 'gato' y 'negro' en el Doc 1 y 'perro' y 'cerca' en el Doc 2 tienen valores más altos, haciéndolas más importantes para la distinción de los documentos.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
