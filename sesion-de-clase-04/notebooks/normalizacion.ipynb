{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T20:30:38.553918Z",
     "start_time": "2025-09-09T20:30:38.549645Z"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "import spacy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/erichuiza/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T20:30:11.155270Z",
     "start_time": "2025-09-09T20:30:08.918084Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m spacy download es_core_news_sm",
   "id": "3dfb8d4fbf40a48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m56.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: es-core-news-sm\r\n",
      "Successfully installed es-core-news-sm-3.8.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('es_core_news_sm')\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T20:31:06.805319Z",
     "start_time": "2025-09-09T20:31:06.670345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stop_words = set(nltk.corpus.stopwords.words('spanish'))"
   ],
   "id": "deed73135fa2aebc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T20:35:35.066694Z",
     "start_time": "2025-09-09T20:35:35.063677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normaliza un texto largo aplicando lematización, eliminación de stopwords y puntuación.\n",
    "\n",
    "    Args:\n",
    "        text: El texto de entrada a normalizar.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de palabras normalizadas.\n",
    "    \"\"\"\n",
    "    # Conversión a minúsculas y eliminación de puntuación\n",
    "    # Esta línea simplifica el texto antes de procesarlo\n",
    "    processed_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "\n",
    "    # 2. Tokenización y Lematización con spaCy\n",
    "    doc = nlp(processed_text)\n",
    "\n",
    "    # 3. Eliminar stopwords y palabras que no son alfabéticas\n",
    "    normalized_words = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and token.text not in stop_words\n",
    "    ]\n",
    "\n",
    "    return normalized_words"
   ],
   "id": "b071005e33b8dfeb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T20:35:55.263667Z",
     "start_time": "2025-09-09T20:35:55.247560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "long_text = \"\"\"\n",
    "Las computadoras están corriendo muy rápido y son importantes para el análisis de los datos.\n",
    "Nosotros estábamos analizando unos datos interesantes.\n",
    "\"\"\"\n",
    "\n",
    "normalized_list = normalize_text(long_text)\n",
    "\n",
    "# Muestra el texto original y el texto normalizado\n",
    "print(\"Texto original:\")\n",
    "print(long_text)\n",
    "print(\"\\n--- Texto Normalizado (lista de palabras) ---\")\n",
    "print(normalized_list)"
   ],
   "id": "d594534a9c01cff8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "\n",
      "Las computadoras están corriendo muy rápido y son importantes para el análisis de los datos.\n",
      "Nosotros estábamos analizando unos datos interesantes.\n",
      "\n",
      "\n",
      "--- Texto Normalizado (lista de palabras) ---\n",
      "['computadora', 'correr', 'rápido', 'importante', 'análisis', 'dato', 'analizar', 'dato', 'interesante']\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
